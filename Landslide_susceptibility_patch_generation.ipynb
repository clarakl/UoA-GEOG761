{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YvgAvzZJ8biMf_rGB7v2OFJOmyMK4_oO",
      "authorship_tag": "ABX9TyPO/uC3Be+4F6NpmqZMbCWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clarakl/UoA-GEOG761/blob/main/Landslide_susceptibility_patch_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data check\n"
      ],
      "metadata": {
        "id": "_mDBXMpHw8TA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJk3IkDfwRpX"
      },
      "outputs": [],
      "source": [
        "# check_data.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/GEOG761 Machine Learning for Remote Sensing/Group project landslide susceptibility/landslides_with_variables_fixed1.csv\"   # CHANGE to your CSV path\n",
        "\n",
        "def infer_date_col(df):\n",
        "    # try common names\n",
        "    for name in ['date','Date','event_date','acq_date','acquisition_date','year']:\n",
        "        if name in df.columns:\n",
        "            return name\n",
        "    # find columns that look like dates\n",
        "    for col in df.columns:\n",
        "        if np.issubdtype(df[col].dtype, np.datetime64):\n",
        "            return col\n",
        "        if df[col].astype(str).str.match(r'\\d{4}-\\d{2}-\\d{2}').any():\n",
        "            return col\n",
        "    return None\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        raise FileNotFoundError(f\"{CSV_PATH} not found. Put your CSV in data/ and update CSV_PATH.\")\n",
        "\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    print(\"Loaded CSV:\", CSV_PATH)\n",
        "    print(\"Shape:\", df.shape)\n",
        "    print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "    # check lat/lon\n",
        "    lat_col = None\n",
        "    lon_col = None\n",
        "    for c in df.columns:\n",
        "        if c.lower() in ['latitude','lat','y']:\n",
        "            lat_col = c\n",
        "        if c.lower() in ['longitude','lon','lng','x']:\n",
        "            lon_col = c\n",
        "    print(\"Inferred lat column:\", lat_col)\n",
        "    print(\"Inferred lon column:\", lon_col)\n",
        "    if lat_col is None or lon_col is None:\n",
        "        print(\"WARNING: Lat/Lon columns not clearly found. Please provide column names for latitude and longitude.\")\n",
        "    else:\n",
        "        print(\"Lat/Lon sample (first 5):\")\n",
        "        print(df[[lat_col, lon_col]].head())\n",
        "\n",
        "    # infer date\n",
        "    date_col = infer_date_col(df)\n",
        "    print(\"Inferred date column:\", date_col)\n",
        "    if date_col:\n",
        "        try:\n",
        "            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "            print(\"Date conversion done. Missing dates:\", df[date_col].isna().sum())\n",
        "            # print yearly distribution\n",
        "            df['year'] = df[date_col].dt.year\n",
        "            print(\"Year counts (top):\")\n",
        "            print(df['year'].value_counts().sort_index())\n",
        "        except Exception as e:\n",
        "            print(\"Could not parse dates:\", e)\n",
        "    else:\n",
        "        print(\"No date column inferred. You'll need to provide an acquisition date column or supply imagery filtered by date externally.\")\n",
        "\n",
        "    # check for label column\n",
        "    potential_labels = [c for c in df.columns if c.lower() in ['landslide','valid landslide','label','target','y','class']]\n",
        "    print(\"Potential label columns:\", potential_labels)\n",
        "    if potential_labels:\n",
        "        label = potential_labels[0]\n",
        "        print(\"Label distribution:\")\n",
        "        print(df[label].value_counts(dropna=False))\n",
        "\n",
        "    # quick spatial bounds\n",
        "    if lat_col and lon_col:\n",
        "        minlat, maxlat = df[lat_col].min(), df[lat_col].max()\n",
        "        minlon, maxlon = df[lon_col].min(), df[lon_col].max()\n",
        "        print(f\"Spatial extent: lat [{minlat:.4f}, {maxlat:.4f}], lon [{minlon:.4f}, {maxlon:.4f}]\")\n",
        "\n",
        "    print(\"\\nIf you have imagery, place it under data/imagery/ as GeoTIFFs named with a date or keep a metadata file mapping tiles to dates.\")\n",
        "    print(\"Next (Step 2) I will show how to build image patches around each lat/lon using rasterio, and how to filter imagery by date so we only use imagery prior to the landslide event.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract patches\n",
        "\n",
        "How it works:\n",
        "\n",
        "- Sentinel-2 SR provides L2A reflectance at 10 m.\n",
        "\n",
        "- The .median() composite merges cloud-free pixels from 2019–2022.\n",
        "\n",
        "- Each point.buffer(464 m) yields a ~928 m window around the coordinate.\n",
        "\n",
        "- Patches are exported to Google Drive → “GEE_Landslide_Patches” folder."
      ],
      "metadata": {
        "id": "uhQn06w-xGEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up GEE API\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='clara-geog761-tryout-1')"
      ],
      "metadata": {
        "id": "ZpGem5uixJNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will transform the numerical long-lat  into GEE points that are georeferenced. The cell after this visualizes these points."
      ],
      "metadata": {
        "id": "36revHaS9-l5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/GEOG761 Machine Learning for Remote Sensing/Group project landslide susceptibility/landslides_with_variables_fixed1.csv\")"
      ],
      "metadata": {
        "id": "OppF2g-BxgQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to ee.FeatureCollection\n",
        "def row_to_feature(row):\n",
        "    geom = ee.Geometry.Point(float(row['Longitude']), float(row['Latitude']))\n",
        "    # Make sure 'Valid Landslide' is integer and has no nulls for filtering\n",
        "    label = row['Valid Landslide']\n",
        "    if pd.notna(label):\n",
        "        return ee.Feature(geom, {'id': int(row.name), 'label': int(label)})\n",
        "    else:\n",
        "      print(\"AAAAAAAAAAAAAA\")\n",
        "    return None\n",
        "\n",
        "features = [row_to_feature(r) for _, r in df.iterrows()]\n",
        "features = [f for f in features if f is not None] # Remove null features\n",
        "fc = ee.FeatureCollection(features)\n",
        "\n",
        "print(\"Feature collection created with\", fc.size().getInfo(), \"points.\")"
      ],
      "metadata": {
        "id": "ecqDSO6BV5iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import geemap\n",
        "\n",
        "# Create an interactive map\n",
        "Map = geemap.Map()\n",
        "\n",
        "# Define visualization parameters (e.g., color the points red)\n",
        "vis_params = {'color': 'red'}\n",
        "\n",
        "# Add the FeatureCollection to the map\n",
        "Map.addLayer(fc, vis_params, 'Landslide Points')\n",
        "\n",
        "# Center the map view on your points with a zoom level of 8\n",
        "Map.centerObject(fc, 8)\n",
        "\n",
        "# Display the map\n",
        "Map"
      ],
      "metadata": {
        "id": "E22ApzGJ-Ok6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentinel-2 Level-2A (Surface Reflectance)\n",
        "collection = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "              .filterDate('2022-01-01', '2022-12-31')\n",
        "              .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 10)))\n",
        "\n",
        "median_image = collection.median().select(['B2','B3','B4','B8','B11','B12'])  # Blue, Green, Red, NIR"
      ],
      "metadata": {
        "id": "WBpbnCzyxWcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate area needed by taking the mean maximum area from all of the landslides"
      ],
      "metadata": {
        "id": "_LY8E4keIwf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define patch size\n",
        "df['Area Maximum'] = pd.to_numeric(df['Area Maximum'], errors='coerce')\n",
        "patch_size_m = np.sqrt(df['Area Maximum'].mean(skipna=True) * 10_000)\n",
        "half_width = patch_size_m / 2\n",
        "print(f\"Suggested patch width: {patch_size_m:.0f} meters\")"
      ],
      "metadata": {
        "id": "XN5U5fmv3rTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a smaller collection for the proof-of-concept export.\n",
        "# Define the number of samples to take from each class\n",
        "sample_size = 1250\n",
        "\n",
        "# Filter the collection for each class\n",
        "positives = fc.filter(ee.Filter.eq('label', 1)).limit(sample_size)\n",
        "negatives = fc.filter(ee.Filter.eq('label', 0)).limit(sample_size)\n",
        "\n",
        "# Merge the two limited collections into one\n",
        "fc_limited = positives.merge(negatives)\n",
        "\n",
        "# Shuffle the collection to mix the positive and negative samples\n",
        "fc_limited = fc_limited.randomColumn()\n",
        "fc_limited = fc_limited.sort('random')\n",
        "print(f\"Limited feature collection to {fc_limited.size().getInfo()} points.\")\n",
        "\n",
        "def create_and_tag_patch(feature):\n",
        "    \"\"\"Extracts a patch and sets its ID and label as properties.\"\"\"\n",
        "    patch = median_image.clip(feature.geometry().buffer(half_width).bounds())\n",
        "    # Set() attaches metadata to the image patch for later use\n",
        "    return patch.set({\n",
        "        'id': feature.get('id'),\n",
        "        'label': feature.get('label')\n",
        "    })\n",
        "\n",
        "# Map the new function over the LIMITED collection\n",
        "tagged_image_patches = fc_limited.map(create_and_tag_patch)\n",
        "\n",
        "print(f\"Created a collection of {tagged_image_patches.size().getInfo()} tagged patches.\")"
      ],
      "metadata": {
        "id": "exfKjFNlxx3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure that the patches have their labels\n",
        "\n",
        "print(\"Labels of first 5 patches:\", tagged_image_patches.limit(5).aggregate_array('label').getInfo())\n",
        "print(\"Nr of negative and positive samples:\", tagged_image_patches.aggregate_histogram('label').getInfo())"
      ],
      "metadata": {
        "id": "lsi5AjPd2Ynr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create these patches in GEE.\n"
      ],
      "metadata": {
        "id": "ntSAW7rDBnjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the server-side collection to a client-side list to loop through it.\n",
        "# This is safe for 2,500 items.\n",
        "patch_list = tagged_image_patches.toList(tagged_image_patches.size())\n",
        "\n",
        "# Get the number of patches to process\n",
        "num_patches = patch_list.size().getInfo()"
      ],
      "metadata": {
        "id": "E1PM-PlDQ_Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking what the entries in the list look like to make sure we're on the right track\n",
        "\n",
        "first_patch_info = ee.Image(patch_list.get(0)).getInfo()\n",
        "print(first_patch_info)"
      ],
      "metadata": {
        "id": "zbgKSWGD4k79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the list and create an export task for each patch\n",
        "for i in range(num_patches):\n",
        "    # Get the image patch from the list\n",
        "    image = ee.Image(patch_list.get(i))\n",
        "\n",
        "    # Get its metadata\n",
        "    patch_id = image.get('id').getInfo()\n",
        "    patch_label = image.get('label').getInfo()\n",
        "\n",
        "    # --- This is the key step ---\n",
        "    # Create a descriptive filename that includes the ID and the label.\n",
        "    filename = f\"patch_id_{patch_id}_label_{patch_label}\"\n",
        "\n",
        "    # Define the export task for this single patch\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "        image=image,\n",
        "        description=f'Export_Patch_{patch_id}',  # Each task needs a unique description\n",
        "        folder='GEE_Landslide_Patches',\n",
        "        fileNamePrefix=filename,\n",
        "        scale=10,\n",
        "        fileFormat='GeoTIFF' # GeoTIFF is a standard image format\n",
        "    )\n",
        "    if i%100 == 0:\n",
        "      print(f\"Exporting patch {i+1}/{num_patches}...\")\n",
        "\n",
        "    # Start the task\n",
        "    task.start()\n",
        "\n",
        "print(f\"All {num_patches} tasks have been submitted.\")\n",
        "print(\"Monitor their progress in the 'Tasks' tab of the GEE Code Editor.\")"
      ],
      "metadata": {
        "id": "cAAY0akfRFGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing one patch\n",
        "\n",
        "import rasterio\n",
        "from rasterio.plot import show\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "filepath = '/content/drive/MyDrive/GEE_Landslide_Patches/patch_id_1171_label_1.tif'\n",
        "\n",
        "with rasterio.open(filepath) as src:\n",
        "    # Read the red, green, and blue bands into a 3D array\n",
        "    # Note: Sentinel-2 band numbers might be different, e.g., B4, B3, B2 are often bands 4, 3, 2.\n",
        "    # We'll assume the first three bands are the ones we want for simplicity here.\n",
        "    # You may need to adjust the numbers in read([1, 2, 3])\n",
        "    rgb = src.read([1, 2, 3])\n",
        "\n",
        "    # Function to normalize bands for display\n",
        "    def normalize(array):\n",
        "        array_min, array_max = array.min(), array.max()\n",
        "        return ((array - array_min) / (array_max - array_min))\n",
        "\n",
        "    # Normalize each band to the 0-1 range for proper RGB display\n",
        "    red_normalized = normalize(rgb[0])\n",
        "    green_normalized = normalize(rgb[1])\n",
        "    blue_normalized = normalize(rgb[2])\n",
        "\n",
        "    # Stack the bands back together\n",
        "    rgb_normalized = np.dstack((red_normalized, green_normalized, blue_normalized))\n",
        "\n",
        "    # Display the true-color image\n",
        "    plt.imshow(rgb_normalized)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Ym4rv8DT6566"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "#STOP HERE,  CODE AFTER THIS IS WORK IN PROGRESS FOR THE MODEL ITSELF\n",
        "----"
      ],
      "metadata": {
        "id": "CtkGBgGFE7TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code prepares a machine learning dataset by mapping over a collection of geographic points (fc). For each point, it extracts a corresponding square patch from a satellite image, then adds the point's label (e.g., landslide or not) directly onto the patch as a new data band. To satisfy the GEE exporter's requirement for a single input image, all these individual, labeled patches are stitched together into a temporary virtual image using .mosaic(). Finally, the Export.image function is configured with patchDimensions to cut this virtual image back up into the original patches, saving them in the highly efficient TFRecord format, which is ideal for training deep learning models."
      ],
      "metadata": {
        "id": "Sd_dcxjHFAra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "\n",
        "# --- Assumed Setup ---\n",
        "# This code assumes you have authenticated with Earth Engine and have the\n",
        "# following variables defined from the previous steps:\n",
        "# ee.Initialize()\n",
        "# fc = your ee.FeatureCollection with the 'label' property for each point\n",
        "# collection = your initial ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "# patch_size_m = the width of your patches in meters (e.g., 316)\n",
        "# --- 1. Define Helper Function and Final Image ---\n",
        "# --- 1. Define Helper Function and Final Image ---\n",
        "\n",
        "# This function was defined in a previous step\n",
        "def extract_rectangular_patch(image, feature, half_width_m=100):\n",
        "    \"\"\"Extracts a rectangular patch from an image.\"\"\"\n",
        "    geom = feature.geometry().buffer(half_width_m).bounds()\n",
        "    patch = image.clip(geom)\n",
        "    return patch\n",
        "\n",
        "# Create the median composite image with all the necessary spectral bands\n",
        "bands_to_select = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']\n",
        "median_image = collection.median().select(bands_to_select)\n",
        "half_width = patch_size_m / 2\n",
        "\n",
        "# --- 2. Create Labeled Image Patches ---\n",
        "\n",
        "# Define a function that extracts a patch AND adds the label as a new band.\n",
        "def extract_and_label_patch(feature):\n",
        "    patch = extract_rectangular_patch(median_image, feature, half_width)\n",
        "    # Create a constant image from the 'label' property of the feature.\n",
        "    label_image = ee.Image.constant(feature.get('label')).rename('label')\n",
        "    # Add this new 'label' band to the image patch.\n",
        "    return patch.addBands(label_image)\n",
        "\n",
        "# Filter the collection to only include features that have a non-null 'label' property.\n",
        "filtered_fc = fc.filter(ee.Filter.notNull(['label']))\n",
        "\n",
        "# Now, map the function over the CLEANED collection\n",
        "labeled_patches = ee.ImageCollection(filtered_fc.map(extract_and_label_patch))\n",
        "\n",
        "# This is the key step: convert the collection of patches into a single image.\n",
        "image_to_export = labeled_patches.mosaic()\n",
        "\n",
        "# The rest of your code (mosaic, export, etc.) remains the same, but you may\n",
        "# want to update the export_region to use the filtered collection.\n",
        "export_region = filtered_fc.geometry().bounds()\n",
        "\n",
        "# Calculate the export dimensions based on the patch size and the\n",
        "# satellite's native resolution (10m for Sentinel-2).\n",
        "native_pixel_dimensions = int(patch_size_m / 10)\n",
        "\n",
        "# Define the export task configuration\n",
        "task = ee.batch.Export.image.toDrive(\n",
        "  image=image_to_export,\n",
        "  description='Landslide_TFRecord_Export_Fixed',\n",
        "  folder='GEE_Landslide_Exports',\n",
        "  fileNamePrefix='landslide_data',\n",
        "  region=export_region,\n",
        "  scale=10,\n",
        "  maxPixels=1e13,\n",
        "  fileFormat='TFRecord',\n",
        "  formatOptions={\n",
        "    'patchDimensions': [native_pixel_dimensions, native_pixel_dimensions],\n",
        "    'compressed': True\n",
        "  }\n",
        ")\n",
        "\n",
        "# Start the export task\n",
        "task.start()\n",
        "\n",
        "print(\"Export task started successfully!\")\n",
        "print(\"Monitor the progress in the 'Tasks' tab of the Google Earth Engine Code Editor.\")\n"
      ],
      "metadata": {
        "id": "aAb6THjHJHJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation"
      ],
      "metadata": {
        "id": "wCDAwwdjFkWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Access data in google drive"
      ],
      "metadata": {
        "id": "Oa0nNbU3Fohk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import glob\n",
        "\n",
        "# Get a list of all your TFRecord files. The '*' handles cases where GEE\n",
        "# creates multiple files (e.g., landslide_data-00000.tfrecord, etc.)\n",
        "folder_path = '/content/drive/MyDrive/GEE_Landslide_Exports/'\n",
        "tfrecord_files = glob.glob(f\"{folder_path}*.tfrecord\")\n",
        "\n",
        "print(f\"Found {len(tfrecord_files)} TFRecord files.\")"
      ],
      "metadata": {
        "id": "wiwFZcH_FtDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Decode the binary data for each band in the TFRecord file"
      ],
      "metadata": {
        "id": "B1Vj2bUzF0iE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the big blueprint image is loaded into the drive from GEE, we need to seperate the features from the label. The output type of this operation are **3d tensors** (width, length, bands). The features are the images, the target is the label (hwich is pased as a band).\n"
      ],
      "metadata": {
        "id": "R1QgVDlQERW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- You must define these variables based on your export ---\n",
        "# This is the 'native_pixel_dimensions' variable from your GEE script\n",
        "PATCH_SIZE = 114 # Example: int(patch_size_m / 10)\n",
        "# All the bands you exported, INCLUDING the 'label' band\n",
        "BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12', 'label']\n",
        "\n",
        "def parse_and_split_tfrecord(serialized_example):\n",
        "    \"\"\"Parses a single TFRecord, stacks bands, and splits into features and label.\"\"\"\n",
        "\n",
        "    # 1. Define the feature description (the \"recipe\" for parsing)\n",
        "    # Each band is a 2D image patch of a fixed size.\n",
        "    feature_description = {\n",
        "        band: tf.io.FixedLenFeature([PATCH_SIZE, PATCH_SIZE], tf.float32) for band in BANDS\n",
        "    }\n",
        "\n",
        "    # 2. Parse the input `serialized_example` using the dictionary\n",
        "    parsed_features = tf.io.parse_single_example(serialized_example, feature_description)\n",
        "\n",
        "    # 3. Stack the individual bands into a single 3D tensor\n",
        "    # The order of bands here is critical.\n",
        "    stacked_tensor = tf.stack([parsed_features[b] for b in BANDS], axis=-1)\n",
        "\n",
        "    # 4. Split the tensor into features (image) and label\n",
        "    # Features are all bands except the last one\n",
        "    features = stacked_tensor[:, :, :-1]  # Shape: [PATCH_SIZE, PATCH_SIZE, 6]\n",
        "\n",
        "    # The label is a single value from the last band\n",
        "    label = stacked_tensor[0, 0, -1]\n",
        "\n",
        "    # Ensure the label is an integer (standard for classification)\n",
        "    label = tf.cast(label, tf.int32)\n",
        "\n",
        "    return features, label"
      ],
      "metadata": {
        "id": "AYZqfC4yF8Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview_patches_gee.py\n",
        "import ee, geemap\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# CONFIG\n",
        "# ------------------------------\n",
        "CSV_PATH = \"landslides_with_variables2.csv\"\n",
        "NUM_PREVIEW = 3\n",
        "PATCH_WIDTH_M = 928\n",
        "HALF_PATCH = PATCH_WIDTH_M / 2\n",
        "SCALE = 10\n",
        "MONTHS_WINDOW = 3           # shorter for faster testing\n",
        "END_DATE_GLOBAL = \"2022-12-31\"\n",
        "CLOUD_PROB_THRESH = 40\n",
        "SCL_MASK_VALUES = [3, 8, 9, 10]\n",
        "\n",
        "# ------------------------------\n",
        "# FIXED CLOUD-JOIN FUNCTION\n",
        "# ------------------------------\n",
        "def add_cloudprob_to_s2collection(s2_sr):\n",
        "    cloud_coll = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "    time_filter = ee.Filter.maxDifference(\n",
        "        difference=24 * 60 * 60 * 1000,\n",
        "        leftField='system:time_start',\n",
        "        rightField='system:time_start'\n",
        "    )\n",
        "    joined = ee.Join.saveBest('cloud', 'timeDiff').apply(\n",
        "        primary=s2_sr,\n",
        "        secondary=cloud_coll,\n",
        "        condition=time_filter\n",
        "    )\n",
        "    def merge_cloud(img):\n",
        "        cloud = ee.Image(img.get('cloud'))\n",
        "        return ee.Image(img).addBands(cloud.select('probability'))\n",
        "    return ee.ImageCollection(joined.map(merge_cloud))\n",
        "\n",
        "def mask_clouds(img):\n",
        "    scl = img.select('SCL')\n",
        "    mask_scl = scl.remap(SCL_MASK_VALUES, [0]*len(SCL_MASK_VALUES), 1).eq(1)\n",
        "    img = img.updateMask(mask_scl)\n",
        "    prob = img.select('probability')\n",
        "    mask_cp = prob.lte(CLOUD_PROB_THRESH)\n",
        "    return img.updateMask(mask_cp)\n",
        "\n",
        "# ------------------------------\n",
        "# COMPOSITE BUILDER WITH PROGRESS PRINTS\n",
        "# ------------------------------\n",
        "def make_composite(lon, lat, end_date=END_DATE_GLOBAL, months=MONTHS_WINDOW):\n",
        "    end = ee.Date(end_date)\n",
        "    start = end.advance(-months, 'month')\n",
        "    geom = ee.Geometry.Point([lon, lat]).buffer(HALF_PATCH)\n",
        "    print(f\"  Date range: {start.format('YYYY-MM-dd').getInfo()} → {end.format('YYYY-MM-dd').getInfo()}\")\n",
        "\n",
        "    s2 = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "          .filterDate(start, end)\n",
        "          .filterBounds(geom)\n",
        "          .select(['B2','B3','B4','B8','SCL']))\n",
        "\n",
        "    # Count how many raw images exist before cloud join\n",
        "    count_raw = s2.size().getInfo()\n",
        "    print(f\"  Raw Sentinel-2 SR images found: {count_raw}\")\n",
        "\n",
        "    if count_raw == 0:\n",
        "        return None\n",
        "\n",
        "    s2 = add_cloudprob_to_s2collection(s2)\n",
        "    s2 = s2.map(mask_clouds)\n",
        "\n",
        "    # Count after masking\n",
        "    count_masked = s2.size().getInfo()\n",
        "    print(f\"  Images remaining after masking: {count_masked}\")\n",
        "\n",
        "    # Show a few timestamps\n",
        "    img_list = s2.toList(5)\n",
        "    timestamps = [ee.Image(img_list.get(i)).date().format('YYYY-MM-dd').getInfo()\n",
        "                  for i in range(min(5, count_masked))]\n",
        "    print(\"  Example image dates:\", timestamps)\n",
        "\n",
        "    if count_masked == 0:\n",
        "        return None\n",
        "\n",
        "    comp = s2.median().divide(10000).select(['B2','B3','B4','B8'])\n",
        "    return comp.clip(geom)\n",
        "\n",
        "# ------------------------------\n",
        "# EXTRACT + PREVIEW\n",
        "# ------------------------------\n",
        "def extract_numpy_patch(image, lon, lat):\n",
        "    geom = ee.Geometry.Point([lon, lat]).buffer(HALF_PATCH)\n",
        "    d = image.sampleRectangle(region=geom, defaultValue=0).getInfo()\n",
        "    bands = ['B2','B3','B4','B8']\n",
        "    arrs = [np.array(d[b]) for b in bands if b in d]\n",
        "    if len(arrs) != 4:\n",
        "        return None\n",
        "    return np.stack(arrs, axis=-1)\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "for i, row in df.head(NUM_PREVIEW).iterrows():\n",
        "    lon, lat = row['Longitude'], row['Latitude']\n",
        "    print(f\"\\n[{i}] Building composite for ({lat:.4f}, {lon:.4f}) …\")\n",
        "    comp = make_composite(lon, lat)\n",
        "    if comp is None:\n",
        "        print(\"  ⚠️  No valid images, skipping.\")\n",
        "        continue\n",
        "    patch = extract_numpy_patch(comp, lon, lat)\n",
        "    if patch is None or np.all(patch == 0):\n",
        "        print(\"  ⚠️  Patch empty or invalid.\")\n",
        "        continue\n",
        "\n",
        "    rgb = patch[:, :, [2,1,0]]\n",
        "    vmin, vmax = np.percentile(rgb[rgb>0], (2, 98))\n",
        "    rgb = np.clip((rgb - vmin) / (vmax - vmin), 0, 1)\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(rgb)\n",
        "    plt.title(f\"Sample {i} ({lat:.3f}, {lon:.3f})\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "el6eOZVDx1y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Parameters ---\n",
        "PATCH_WIDTH_M = 928   # from your mean area estimate\n",
        "HALF_PATCH = PATCH_WIDTH_M / 2\n",
        "POINT = ee.Geometry.Point(174.7463, -36.8199)\n",
        "\n",
        "# --- Date range ---\n",
        "start = '2022-09-30'\n",
        "end   = '2022-12-31'\n",
        "\n",
        "# --- Sentinel-2 Surface Reflectance ---\n",
        "collection = (\n",
        "    ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "    .filterBounds(POINT)\n",
        "    .filterDate(start, end)\n",
        "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))\n",
        ")\n",
        "\n",
        "# Cloud/shadow masking function\n",
        "def mask_s2_sr(image):\n",
        "    cloud_prob = image.select('SCL').eq(3).Or(image.select('SCL').eq(8))\n",
        "    snow = image.select('SCL').eq(11)\n",
        "    return image.updateMask(cloud_prob.Not()).updateMask(snow.Not())\n",
        "\n",
        "masked = collection.map(mask_s2_sr)\n",
        "\n",
        "# Median composite\n",
        "median_img = masked.median().select(['B4', 'B3', 'B2', 'B8'])\n",
        "\n",
        "# Clip to a 928m square patch\n",
        "patch_geom = POINT.buffer(HALF_PATCH).bounds()\n",
        "patch_img = median_img.clip(patch_geom)\n",
        "\n",
        "# --- Generate a download URL ---\n",
        "url = patch_img.getDownloadURL({\n",
        "    'scale': 10,           # Sentinel-2 native resolution\n",
        "    'crs': 'EPSG:4326',\n",
        "    'region': patch_geom\n",
        "})\n",
        "\n",
        "print(\"✅ Download URL for the patch:\\n\", url)"
      ],
      "metadata": {
        "id": "vCIsdImb_6Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Map = geemap.Map()\n",
        "Map.centerObject(fc, 10)\n",
        "Map.addLayer(median_image, {'bands':['B4','B3','B2'], 'min':0, 'max':3000}, 'Sentinel-2 pre-2023')\n",
        "Map.addLayer(fc, {}, 'Landslide points')\n",
        "Map\n"
      ],
      "metadata": {
        "id": "86TS76oxyESq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}